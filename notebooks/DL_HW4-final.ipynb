{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IBulAx6a_QtM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBulAx6a_QtM",
        "outputId": "fbdfaa7e-8636-4571-ab20-16fe86670296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There's nothing to tell! He's just some guy I work with!\n",
            "C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
            "All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
            "Wait, does he eat chalk?\n",
            "(They all stare, bemused.)\n",
            "Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n",
            "Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\n",
            "Sounds like a date to me.\n",
            "[Time Lapse]\n",
            "Alright,\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "\n",
        "\"\"\"## Phase 1: Data Preparation\"\"\"\n",
        "\n",
        "def load_friends_dialogue(csv_path, max_lines=None):\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV file {csv_path} not found\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if 'text' not in df.columns:\n",
        "        raise ValueError(\"CSV file must contain a 'text' column\")\n",
        "    df = df.dropna(subset=['text'])\n",
        "    texts = df['text'].astype(str).tolist()\n",
        "    if max_lines:\n",
        "        texts = texts[:max_lines]\n",
        "    full_text = \"\\n\".join(texts)\n",
        "    return full_text\n",
        "\n",
        "text = load_friends_dialogue(\"friends.csv\")\n",
        "print(text[:500])  # Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m7tQIwL9_TWe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7tQIwL9_TWe",
        "outputId": "cb3c47aa-fff0-4b37-ba89-71718825dd6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 92\n"
          ]
        }
      ],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, text):\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
        "        self.vocab_size = len(self.stoi)\n",
        "\n",
        "    def encode(self, s):\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ''.join([self.itos[i] for i in ids])\n",
        "\n",
        "tokenizer = CharTokenizer(text)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K1v9t8iS_YIh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1v9t8iS_YIh",
        "outputId": "806af0f7-28ca-4a8b-89a3-8a30ba474e73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '\"',\n",
              " 4: '#',\n",
              " 5: '$',\n",
              " 6: '%',\n",
              " 7: '&',\n",
              " 8: \"'\",\n",
              " 9: '(',\n",
              " 10: ')',\n",
              " 11: '*',\n",
              " 12: '+',\n",
              " 13: ',',\n",
              " 14: '-',\n",
              " 15: '.',\n",
              " 16: '/',\n",
              " 17: '0',\n",
              " 18: '1',\n",
              " 19: '2',\n",
              " 20: '3',\n",
              " 21: '4',\n",
              " 22: '5',\n",
              " 23: '6',\n",
              " 24: '7',\n",
              " 25: '8',\n",
              " 26: '9',\n",
              " 27: ':',\n",
              " 28: ';',\n",
              " 29: '<',\n",
              " 30: '=',\n",
              " 31: '>',\n",
              " 32: '?',\n",
              " 33: 'A',\n",
              " 34: 'B',\n",
              " 35: 'C',\n",
              " 36: 'D',\n",
              " 37: 'E',\n",
              " 38: 'F',\n",
              " 39: 'G',\n",
              " 40: 'H',\n",
              " 41: 'I',\n",
              " 42: 'J',\n",
              " 43: 'K',\n",
              " 44: 'L',\n",
              " 45: 'M',\n",
              " 46: 'N',\n",
              " 47: 'O',\n",
              " 48: 'P',\n",
              " 49: 'Q',\n",
              " 50: 'R',\n",
              " 51: 'S',\n",
              " 52: 'T',\n",
              " 53: 'U',\n",
              " 54: 'V',\n",
              " 55: 'W',\n",
              " 56: 'X',\n",
              " 57: 'Y',\n",
              " 58: 'Z',\n",
              " 59: '[',\n",
              " 60: ']',\n",
              " 61: '^',\n",
              " 62: '_',\n",
              " 63: '`',\n",
              " 64: 'a',\n",
              " 65: 'b',\n",
              " 66: 'c',\n",
              " 67: 'd',\n",
              " 68: 'e',\n",
              " 69: 'f',\n",
              " 70: 'g',\n",
              " 71: 'h',\n",
              " 72: 'i',\n",
              " 73: 'j',\n",
              " 74: 'k',\n",
              " 75: 'l',\n",
              " 76: 'm',\n",
              " 77: 'n',\n",
              " 78: 'o',\n",
              " 79: 'p',\n",
              " 80: 'q',\n",
              " 81: 'r',\n",
              " 82: 's',\n",
              " 83: 't',\n",
              " 84: 'u',\n",
              " 85: 'v',\n",
              " 86: 'w',\n",
              " 87: 'x',\n",
              " 88: 'y',\n",
              " 89: 'z',\n",
              " 90: '{',\n",
              " 91: '}'}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.itos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mg_81DCO_WqC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg_81DCO_WqC",
        "outputId": "db8484bb-7d61-4605-9ca7-94a18312aa49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset shape: torch.Size([3380818, 32])\n",
            "Validation dataset shape: torch.Size([375647, 32])\n"
          ]
        }
      ],
      "source": [
        "def create_dataset(text, tokenizer, block_size):\n",
        "    data = tokenizer.encode(text)\n",
        "    xs, ys = [], []\n",
        "    for i in range(0, len(data) - block_size):\n",
        "        x = data[i:i+block_size]\n",
        "        y = data[i+1:i+block_size+1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return torch.tensor(xs), torch.tensor(ys)\n",
        "\n",
        "block_size = 32\n",
        "X, Y = create_dataset(text, tokenizer, block_size)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_size = int(0.9 * len(X))\n",
        "X_train, Y_train = X[:train_size], Y[:train_size]\n",
        "X_val, Y_val = X[train_size:], Y[train_size:]\n",
        "print(f\"Training dataset shape: {X_train.shape}\")\n",
        "print(f\"Validation dataset shape: {X_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyKsKJvr_hFI",
      "metadata": {
        "id": "kyKsKJvr_hFI"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Phase 2: Model Definition\"\"\"\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(emb_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 4 * emb_dim),  # Expand: 64 → 256\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * emb_dim, emb_dim),  # 256 → 64\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Add causal mask\n",
        "        if mask is None:\n",
        "            seq_len = x.size(1)\n",
        "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
        "            mask = mask.masked_fill(mask, float('-inf'))\n",
        "        attn_out, _ = self.attn(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, block_size=32, n_layers=6, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_enc = PositionalEncoding(emb_dim, max_len=block_size)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(emb_dim, n_heads) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.head = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MWHF_oqD_nV_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWHF_oqD_nV_",
        "outputId": "9bccfab0-cc3d-4741-807e-6710e0a07226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CtnWh6Im_qQJ",
      "metadata": {
        "id": "CtnWh6Im_qQJ"
      },
      "outputs": [],
      "source": [
        "def get_batch(X, Y, batch_size, device):\n",
        "    idx = torch.randint(0, X.size(0), (batch_size,))\n",
        "    x = X[idx].to(device)\n",
        "    y = Y[idx].to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcJ47DZW_tsj",
      "metadata": {
        "id": "vcJ47DZW_tsj"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train, Y_train, X_val, Y_val, optimizer, scheduler, tokenizer, criterion,\n",
        "                max_iter=5000, eval_interval=100, batch_size=16, device='cpu'):\n",
        "    model.train()\n",
        "    for step in range(1, max_iter + 1):\n",
        "        xb, yb = get_batch(X_train, Y_train, batch_size, device)\n",
        "        logits = model(xb)\n",
        "        B, T, V = logits.shape\n",
        "        loss = criterion(logits.view(B*T, V), yb.view(B*T))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if step % eval_interval == 0:\n",
        "            # Compute validation loss\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_xb, val_yb = get_batch(X_val, Y_val, batch_size, device)\n",
        "                val_logits = model(val_xb)\n",
        "                val_loss = criterion(val_logits.view(B*T, V), val_yb.view(B*T))\n",
        "            model.train()\n",
        "            print(f\"Step {step}/{max_iter} | Train Loss = {loss.item():.4f} | Val Loss = {val_loss.item():.4f}\")\n",
        "            print(\"Sample:\\n\" + generate_text(model, tokenizer, max_length=100, device=device, temperature=0.7, top_k=50))\n",
        "            print(\"-\" * 50)\n",
        "            # Save checkpoint\n",
        "            torch.save(model.state_dict(), f\"model_step_{step}.pt\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(\n",
        "    model, tokenizer, block_size, max_length=100, device='cpu',\n",
        "    start_text=\"\\n\", temperature=1.0, top_k=None, beam_width=None\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    if beam_width is None:\n",
        "        # === Sampling Mode ===\n",
        "        context = torch.tensor([tokenizer.encode(start_text)], dtype=torch.long).to(device)\n",
        "        for _ in range(max_length):\n",
        "            if context.size(1) > block_size:\n",
        "                context = context[:, -block_size:]\n",
        "            logits = model(context)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            context = torch.cat([context, next_id], dim=1)\n",
        "        return tokenizer.decode(context[0].tolist())\n",
        "\n",
        "    else:\n",
        "        # === Beam Search Mode ===\n",
        "        beam_width = int(beam_width)\n",
        "        beams = [(tokenizer.encode(start_text), 0.0)]  # (tokens, score)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            candidates = []\n",
        "            for tokens, score in beams:\n",
        "                context = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "                if context.size(1) > block_size:\n",
        "                    context = context[:, -block_size:]\n",
        "                logits = model(context)\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "                probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                top_probs, top_idxs = torch.topk(probs, beam_width, dim=-1)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    new_token = top_idxs[0, i].item()\n",
        "                    new_score = score + top_probs[0, i].item()\n",
        "                    candidates.append((tokens + [new_token], new_score))\n",
        "\n",
        "            # Keep top-k beams\n",
        "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            beams = candidates[:beam_width]\n",
        "\n",
        "        best_seq = beams[0][0]\n",
        "        return tokenizer.decode(best_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C_QftxO_9hro",
      "metadata": {
        "id": "C_QftxO_9hro"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iter = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layers = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X-hFRP4O_1SC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-hFRP4O_1SC",
        "outputId": "c782385d-d8e2-4410-b006-2682c5b33c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyGPT has 511836 parameters!\n"
          ]
        }
      ],
      "source": [
        "model = GPT(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    emb_dim=n_embd,\n",
        "    block_size=block_size,\n",
        "    n_layers=n_layers,\n",
        "    n_heads=n_head\n",
        ").to(device)\n",
        "\n",
        "print(f\"MyGPT has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AzTDdKOX7Rzf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzTDdKOX7Rzf",
        "outputId": "ac3f46d4-112e-461e-8379-cecc8afe0092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100/5000 | Train Loss = 2.5683 | Val Loss = 2.5411\n",
            "Sample:\n",
            "Jou t chiind te thit.. l athay yo\n",
            "--------------------------------------------------\n",
            "Step 200/5000 | Train Loss = 2.1660 | Val Loss = 2.4062\n",
            "Sample:\n",
            "herey am y athimeble tston a I an\n",
            "--------------------------------------------------\n",
            "Step 300/5000 | Train Loss = 2.2227 | Val Loss = 2.1287\n",
            "Sample:\n",
            "eright thame so do ser thoull, th\n",
            "--------------------------------------------------\n",
            "Step 400/5000 | Train Loss = 1.9500 | Val Loss = 1.9793\n",
            "Sample:\n",
            "thedo ony. Rossss.\n",
            "Oh, you was lo\n",
            "--------------------------------------------------\n",
            "Step 500/5000 | Train Loss = 1.9687 | Val Loss = 1.9637\n",
            "Sample:\n",
            "er him achen lis ry his a rindal,\n",
            "--------------------------------------------------\n",
            "Step 600/5000 | Train Loss = 1.8144 | Val Loss = 1.8263\n",
            "Sample:\n",
            "nica the the's lat gonna marron.\n",
            "\n",
            "--------------------------------------------------\n",
            "Step 700/5000 | Train Loss = 1.9174 | Val Loss = 1.9006\n",
            "Sample:\n",
            "'s gain a the me? What for teng. \n",
            "--------------------------------------------------\n",
            "Step 800/5000 | Train Loss = 1.7863 | Val Loss = 1.9829\n",
            "Sample:\n",
            "guain bale. I sant the am the pee\n",
            "--------------------------------------------------\n",
            "Step 900/5000 | Train Loss = 1.8380 | Val Loss = 1.6701\n",
            "Sample:\n",
            "est are this hip taite toelle?\n",
            "Ye\n",
            "--------------------------------------------------\n",
            "Step 1000/5000 | Train Loss = 1.7854 | Val Loss = 1.7595\n",
            "Sample:\n",
            "ing be the the elsh has gepar and\n",
            "--------------------------------------------------\n",
            "Step 1100/5000 | Train Loss = 1.6854 | Val Loss = 1.7898\n",
            "Sample:\n",
            "hanged the bare stay though that \n",
            "--------------------------------------------------\n",
            "Step 1200/5000 | Train Loss = 1.8318 | Val Loss = 1.7516\n",
            "Sample:\n",
            "he can.\n",
            "No.\n",
            "You mean that. The ap\n",
            "--------------------------------------------------\n",
            "Step 1300/5000 | Train Loss = 1.4913 | Val Loss = 1.7668\n",
            "Sample:\n",
            "s.\n",
            "Yeah, get where to sup tund to\n",
            "--------------------------------------------------\n",
            "Step 1400/5000 | Train Loss = 1.7982 | Val Loss = 1.7211\n",
            "Sample:\n",
            " dringer. I maredy I'm says doing\n",
            "--------------------------------------------------\n",
            "Step 1500/5000 | Train Loss = 1.5761 | Val Loss = 1.8329\n",
            "Sample:\n",
            "this shaturd the are where breatt\n",
            "--------------------------------------------------\n",
            "Step 1600/5000 | Train Loss = 1.5898 | Val Loss = 1.5582\n",
            "Sample:\n",
            "reat?\n",
            "Uh, you should way noth a s\n",
            "--------------------------------------------------\n",
            "Step 1700/5000 | Train Loss = 1.6386 | Val Loss = 1.7078\n",
            "Sample:\n",
            "ow don't believe you love ash loo\n",
            "--------------------------------------------------\n",
            "Step 1800/5000 | Train Loss = 1.5763 | Val Loss = 1.6168\n",
            "Sample:\n",
            " on it you for for that in someth\n",
            "--------------------------------------------------\n",
            "Step 1900/5000 | Train Loss = 1.6196 | Val Loss = 1.8018\n",
            "Sample:\n",
            "y now was good so the fact belout\n",
            "--------------------------------------------------\n",
            "Step 2000/5000 | Train Loss = 1.5265 | Val Loss = 1.5896\n",
            "Sample:\n",
            "one didn't the great, look on my \n",
            "--------------------------------------------------\n",
            "Step 2100/5000 | Train Loss = 1.5151 | Val Loss = 1.8061\n",
            "Sample:\n",
            " and that the minthing stold up w\n",
            "--------------------------------------------------\n",
            "Step 2200/5000 | Train Loss = 1.6400 | Val Loss = 1.6663\n",
            "Sample:\n",
            "t?\n",
            "I was that. I mean suche. It's\n",
            "--------------------------------------------------\n",
            "Step 2300/5000 | Train Loss = 1.6882 | Val Loss = 1.6601\n",
            "Sample:\n",
            "ing exper Joess enters it this si\n",
            "--------------------------------------------------\n",
            "Step 2400/5000 | Train Loss = 1.5790 | Val Loss = 1.6071\n",
            "Sample:\n",
            "arectsed you got were you gonna t\n",
            "--------------------------------------------------\n",
            "Step 2500/5000 | Train Loss = 1.6013 | Val Loss = 1.5279\n",
            "Sample:\n",
            "r a dances up the one girl for ma\n",
            "--------------------------------------------------\n",
            "Step 2600/5000 | Train Loss = 1.4875 | Val Loss = 1.5102\n",
            "Sample:\n",
            "\n",
            "Oh yeah, I ming should might mon\n",
            "--------------------------------------------------\n",
            "Step 2700/5000 | Train Loss = 1.5581 | Val Loss = 1.5737\n",
            "Sample:\n",
            "e up friend.\n",
            "Yeah.\n",
            "Okay phone, yo\n",
            "--------------------------------------------------\n",
            "Step 2800/5000 | Train Loss = 1.4385 | Val Loss = 1.6056\n",
            "Sample:\n",
            "and borrth off you!\n",
            "Okay, how don\n",
            "--------------------------------------------------\n",
            "Step 2900/5000 | Train Loss = 1.5942 | Val Loss = 1.6007\n",
            "Sample:\n",
            "ckes in the seeping the fist beca\n",
            "--------------------------------------------------\n",
            "Step 3000/5000 | Train Loss = 1.4068 | Val Loss = 1.4589\n",
            "Sample:\n",
            "f the look for a better past next\n",
            "--------------------------------------------------\n",
            "Step 3100/5000 | Train Loss = 1.5315 | Val Loss = 1.4085\n",
            "Sample:\n",
            "dawite, show uh was a cleating, h\n",
            "--------------------------------------------------\n",
            "Step 3200/5000 | Train Loss = 1.6884 | Val Loss = 1.4941\n",
            "Sample:\n",
            "o, no what did you talk to about \n",
            "--------------------------------------------------\n",
            "Step 3300/5000 | Train Loss = 1.5693 | Val Loss = 1.5698\n",
            "Sample:\n",
            "\n",
            "Ow, never the brothle? It him th\n",
            "--------------------------------------------------\n",
            "Step 3400/5000 | Train Loss = 1.4647 | Val Loss = 1.4800\n",
            "Sample:\n",
            "y me, but it's gonna a too and an\n",
            "--------------------------------------------------\n",
            "Step 3500/5000 | Train Loss = 1.3122 | Val Loss = 1.4901\n",
            "Sample:\n",
            " theweren who at the cotts to mas\n",
            "--------------------------------------------------\n",
            "Step 3600/5000 | Train Loss = 1.3030 | Val Loss = 1.4219\n",
            "Sample:\n",
            "e.\n",
            "(Chandler and Phoebe seess for\n",
            "--------------------------------------------------\n",
            "Step 3700/5000 | Train Loss = 1.4765 | Val Loss = 1.6115\n",
            "Sample:\n",
            "the someone.\n",
            "Whoa-where what to d\n",
            "--------------------------------------------------\n",
            "Step 3800/5000 | Train Loss = 1.3501 | Val Loss = 1.3260\n",
            "Sample:\n",
            "h.\n",
            "How and I can do this was pail\n",
            "--------------------------------------------------\n",
            "Step 3900/5000 | Train Loss = 1.4140 | Val Loss = 1.9227\n",
            "Sample:\n",
            "er apartment and there weird, and\n",
            "--------------------------------------------------\n",
            "Step 4000/5000 | Train Loss = 1.6566 | Val Loss = 1.4171\n",
            "Sample:\n",
            "bout.\n",
            "So me-this an may offfend m\n",
            "--------------------------------------------------\n",
            "Step 4100/5000 | Train Loss = 1.5241 | Val Loss = 1.5319\n",
            "Sample:\n",
            "married. Would you know what?!\n",
            "Wh\n",
            "--------------------------------------------------\n",
            "Step 4200/5000 | Train Loss = 1.3591 | Val Loss = 1.3845\n",
            "Sample:\n",
            "ke they is only friends, and a li\n",
            "--------------------------------------------------\n",
            "Step 4300/5000 | Train Loss = 1.3766 | Val Loss = 1.4465\n",
            "Sample:\n",
            "in the these dreams and that how \n",
            "--------------------------------------------------\n",
            "Step 4400/5000 | Train Loss = 1.4334 | Val Loss = 1.3128\n",
            "Sample:\n",
            "Hand now a bother and said.\n",
            "Oh yo\n",
            "--------------------------------------------------\n",
            "Step 4500/5000 | Train Loss = 1.4810 | Val Loss = 1.4329\n",
            "Sample:\n",
            "rs to birthday, I like I-I don't \n",
            "--------------------------------------------------\n",
            "Step 4600/5000 | Train Loss = 1.4532 | Val Loss = 1.2649\n",
            "Sample:\n",
            "rtment.\n",
            "(The barth too! And they \n",
            "--------------------------------------------------\n",
            "Step 4700/5000 | Train Loss = 1.5555 | Val Loss = 1.5152\n",
            "Sample:\n",
            "irth thing.\n",
            "You can't make you, w\n",
            "--------------------------------------------------\n",
            "Step 4800/5000 | Train Loss = 1.4576 | Val Loss = 1.3583\n",
            "Sample:\n",
            "d you know the mandler on the hou\n",
            "--------------------------------------------------\n",
            "Step 4900/5000 | Train Loss = 1.4351 | Val Loss = 1.4987\n",
            "Sample:\n",
            "ng with Joey and first, are they \n",
            "--------------------------------------------------\n",
            "Step 5000/5000 | Train Loss = 1.4371 | Val Loss = 1.3516\n",
            "Sample:\n",
            " you really for the cotie.\n",
            "Yes! I\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iter)\n",
        "\n",
        "train_model(\n",
        "    model=model,\n",
        "    X_train=X_train,\n",
        "    Y_train=Y_train,\n",
        "    X_val=X_val,\n",
        "    Y_val=Y_val,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    tokenizer=tokenizer,\n",
        "    criterion=criterion,\n",
        "    max_iter=max_iter,\n",
        "    eval_interval=eval_interval,\n",
        "    batch_size=batch_size,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PF8YiigQQqLd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF8YiigQQqLd",
        "outputId": "ae6c5a9e-4f42-420a-de54-df0bfaad6f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling:\n",
            "their fored.\n",
            "Yeah, three and star\n"
          ]
        }
      ],
      "source": [
        "print(\"Sampling:\")\n",
        "print(generate_text(model, tokenizer, block_size, start_text=\"Joey: \", temperature=0.8, top_k=5, max_length=200, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6l7bOSwXQuNI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l7bOSwXQuNI",
        "outputId": "5c72b8bd-a9f6-40ca-b12b-80fdffb423c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Beam Search:\n",
            "Monica: Monica and Chandler and Rachel's, Joey and Rachel's, Ross is and Rachel's, Ross isn't thinking about the couch of there and there's a little back of there.\n",
            "Oh my God, you're gonna get to be there.\n",
            "Oh,\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBeam Search:\")\n",
        "print(generate_text(model, tokenizer, block_size, start_text=\"Monica: \", beam_width=3, max_length=200, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4P2uxAdW9Zlo",
      "metadata": {
        "id": "4P2uxAdW9Zlo"
      },
      "outputs": [],
      "source": [
        "def load_gpt_model_from_checkpoint(\n",
        "    checkpoint_path,\n",
        "    vocab_size,\n",
        "    emb_dim=64,\n",
        "    block_size=32,\n",
        "    n_layers=6,\n",
        "    n_heads=4,\n",
        "    device='cpu'\n",
        "):\n",
        "    model = GPT(\n",
        "        vocab_size=vocab_size,\n",
        "        emb_dim=emb_dim,\n",
        "        block_size=block_size,\n",
        "        n_layers=n_layers,\n",
        "        n_heads=n_heads\n",
        "    ).to(device)\n",
        "\n",
        "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v4K5c-hL99ut",
      "metadata": {
        "id": "v4K5c-hL99ut"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"gpt_friends.pth\"\n",
        "loaded_model = load_gpt_model_from_checkpoint(\n",
        "    checkpoint_path,\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    emb_dim=n_embd,\n",
        "    block_size=block_size,\n",
        "    n_layers=n_layers,\n",
        "    n_heads=n_head,\n",
        "    device=device\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "67ZNLgRPmy-V",
        "pkqBIn1E8kic",
        "wUpD6jl18egT",
        "izhF8zsCCRVp"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mygpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
